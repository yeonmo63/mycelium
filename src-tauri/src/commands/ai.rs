use crate::commands::config::get_gemini_api_key;
use crate::commands::config::get_naver_keys;
use crate::db::{Customer, DbPool};
use serde::{Deserialize, Serialize};
use tauri::{command, AppHandle, State};

#[derive(Debug, Serialize, Deserialize)]
pub struct NaverSearchResult {
    pub items: Vec<NaverItem>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct NaverItem {
    pub title: String,
    pub link: String,
    pub description: String,
    pub bloggername: Option<String>,
    pub postdate: String,
}

#[command]
pub async fn fetch_naver_search(app: AppHandle, query: String) -> Result<Vec<NaverItem>, String> {
    let (client_id, client_secret) = get_naver_keys(&app);

    let url = format!(
        "https://openapi.naver.com/v1/search/blog.json?query={}&display=10&sort=sim",
        urlencoding::encode(&query)
    );

    let client = reqwest::Client::new();
    let res = client
        .get(&url)
        .header("X-Naver-Client-Id", client_id)
        .header("X-Naver-Client-Secret", client_secret)
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?;

    if !res.status().is_success() {
        return Err(format!("Naver API Error: {}", res.status()));
    }

    let search_result: NaverSearchResult = res
        .json()
        .await
        .map_err(|e| format!("Parse failed: {}", e))?;

    Ok(search_result.items)
}

#[command]
pub async fn call_gemini_ai(app: AppHandle, prompt: String) -> Result<String, String> {
    let api_key = get_gemini_api_key(&app).ok_or("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")?;
    call_gemini_ai_internal(&api_key, &prompt).await
}

pub async fn call_gemini_ai_internal(api_key: &str, prompt: &str) -> Result<String, String> {
    let clean_key = api_key.trim().trim_matches(|c: char| c == '"' || c == '\'');
    let client = reqwest::Client::new();

    // 1. Dynamic Model Discovery
    let mut models_to_try = Vec::new();

    // Try to fetch available models
    let list_url = format!(
        "https://generativelanguage.googleapis.com/v1beta/models?key={}",
        clean_key
    );

    if let Ok(resp) = client.get(&list_url).send().await {
        if resp.status().is_success() {
            if let Ok(json) = resp.json::<serde_json::Value>().await {
                if let Some(models) = json["models"].as_array() {
                    for model in models {
                        if let Some(name) = model["name"].as_str() {
                            let supported = model["supportedGenerationMethods"]
                                .as_array()
                                .map(|methods| {
                                    methods
                                        .iter()
                                        .any(|m| m.as_str() == Some("generateContent"))
                                })
                                .unwrap_or(false);

                            if supported && name.contains("gemini") {
                                let short_name = name.trim_start_matches("models/");
                                models_to_try.push(("v1beta".to_string(), short_name.to_string()));
                            }
                        }
                    }
                }
            }
        }
    }

    // 2. Fallback / Priority Sorting
    if models_to_try.is_empty() {
        models_to_try = vec![
            ("v1".to_string(), "gemini-1.5-flash".to_string()),
            ("v1beta".to_string(), "gemini-1.5-flash".to_string()),
            ("v1".to_string(), "gemini-1.5-flash-8b".to_string()),
            ("v1beta".to_string(), "gemini-1.5-pro-latest".to_string()),
        ];
    } else {
        models_to_try.sort_by(|a, b| {
            let a_score = if a.1.contains("flash") {
                2
            } else if a.1.contains("pro") {
                1
            } else {
                0
            };
            let b_score = if b.1.contains("flash") {
                2
            } else if b.1.contains("pro") {
                1
            } else {
                0
            };
            b_score.cmp(&a_score)
        });
    }

    let mut errors = Vec::new();

    for (version, model) in models_to_try {
        let url = format!(
            "https://generativelanguage.googleapis.com/{}/models/{}:generateContent?key={}",
            version, model, clean_key
        );

        let body = serde_json::json!({
            "contents": [{ "parts": [{ "text": prompt }] }]
        });

        let resp = match client.post(&url).json(&body).send().await {
            Ok(r) => r,
            Err(e) => {
                errors.push(format!("Network Error ({}): {}", model, e));
                continue;
            }
        };

        if resp.status().is_success() {
            let json: serde_json::Value = resp.json().await.unwrap_or_default();
            if let Some(content) = json["candidates"][0]["content"]["parts"][0]["text"].as_str() {
                let cleaned = content
                    .trim()
                    .trim_start_matches("```json")
                    .trim_start_matches("```")
                    .trim_end_matches("```")
                    .trim();
                return Ok(cleaned.to_string());
            } else {
                errors.push(format!("Empty response from {}", model));
            }
        } else {
            let status = resp.status();
            let error_text = resp.text().await.unwrap_or_default();

            if status == reqwest::StatusCode::TOO_MANY_REQUESTS {
                return Err("AI_QUOTA_EXCEEDED: Gemini AI ì‚¬ìš© í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.\n\nì¼ì¼ ë¬´ë£Œ í•œë„ê°€ ì†Œì§„ë˜ì—ˆê±°ë‚˜, ë¶„ë‹¹ ìš”ì²­ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜, API í‚¤ ì„¤ì •ì—ì„œ ìœ ë£Œ í”Œëœìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œí•˜ì„¸ìš”.".to_string());
            }

            if status == reqwest::StatusCode::FORBIDDEN {
                if error_text.contains("quota")
                    || error_text.contains("limit")
                    || error_text.contains("exceeded")
                {
                    return Err("AI_QUOTA_EXCEEDED: Gemini AI í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\n\nAPI í‚¤ì˜ ì‚¬ìš© í•œë„ê°€ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤. Google AI Studioì—ì„œ ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•˜ê±°ë‚˜, ìƒˆë¡œìš´ API í‚¤ë¥¼ ë°œê¸‰ë°›ìœ¼ì„¸ìš”.".to_string());
                }
            }

            errors.push(format!(
                "API Error ({}): {} - {}",
                model, status, error_text
            ));

            if status == reqwest::StatusCode::TOO_MANY_REQUESTS {
                break;
            }
        }
    }

    Err(format!("AI ëª¨ë¸ ì—°ê²° ì‹¤íŒ¨:\n{}", errors.join("\n")))
}

#[derive(Serialize, Deserialize, Debug)]
pub struct ParsedBusinessCard {
    pub name: Option<String>,
    pub mobile: Option<String>,
    pub phone: Option<String>,
    pub email: Option<String>,
    pub company: Option<String>,
    pub job_title: Option<String>,
    pub address: Option<String>,
    pub memo: Option<String>,
}

pub async fn call_gemini_vision_ai(
    api_key: &str,
    prompt: &str,
    image_base64: &str,
    mime_type: &str,
) -> Result<String, String> {
    let clean_key = api_key.trim().trim_matches(|c: char| c == '"' || c == '\'');
    let client = reqwest::Client::new();

    let mut models_to_try = Vec::new();
    let list_url = format!(
        "https://generativelanguage.googleapis.com/v1beta/models?key={}",
        clean_key
    );

    if let Ok(resp) = client.get(&list_url).send().await {
        if resp.status().is_success() {
            if let Ok(json) = resp.json::<serde_json::Value>().await {
                if let Some(models) = json["models"].as_array() {
                    for model in models {
                        if let Some(name) = model["name"].as_str() {
                            let supported = model["supportedGenerationMethods"]
                                .as_array()
                                .map(|methods| {
                                    methods
                                        .iter()
                                        .any(|m| m.as_str() == Some("generateContent"))
                                })
                                .unwrap_or(false);

                            if supported && name.contains("gemini") {
                                let short_name = name.trim_start_matches("models/");
                                if short_name.contains("flash") || short_name.contains("pro") {
                                    models_to_try
                                        .push(("v1beta".to_string(), short_name.to_string()));
                                }
                            }
                        }
                    }
                }
            }
        }
    }

    if models_to_try.is_empty() {
        models_to_try = vec![
            ("v1".to_string(), "gemini-1.5-flash".to_string()),
            ("v1beta".to_string(), "gemini-1.5-flash".to_string()),
            ("v1".to_string(), "gemini-1.5-flash-8b".to_string()),
            ("v1beta".to_string(), "gemini-1.5-pro-latest".to_string()),
        ];
    } else {
        models_to_try.sort_by(|a, b| {
            let get_score = |m: &str| {
                if m.contains("1.5-flash") && !m.contains("8b") {
                    10
                } else if m.contains("1.5-flash-8b") {
                    8
                } else if m.contains("2.0-flash") && !m.contains("exp") {
                    7
                } else if m.contains("pro") {
                    5
                } else if m.contains("exp") {
                    1
                } else {
                    3
                }
            };
            get_score(&b.1).cmp(&get_score(&a.1))
        });
    }

    let mut errors = Vec::new();

    for (version, model) in models_to_try {
        let url = format!(
            "https://generativelanguage.googleapis.com/{}/models/{}:generateContent?key={}",
            version, model, clean_key
        );

        let body = serde_json::json!({
            "contents": [{
                "parts": [
                    { "text": prompt },
                    {
                        "inline_data": {
                            "mime_type": mime_type,
                            "data": image_base64
                        }
                    }
                ]
            }]
        });

        let resp = match client.post(&url).json(&body).send().await {
            Ok(r) => r,
            Err(e) => {
                errors.push(format!("Network Error ({}): {}", model, e));
                continue;
            }
        };

        if resp.status().is_success() {
            let json: serde_json::Value = resp.json().await.map_err(|e| e.to_string())?;
            if let Some(content) = json["candidates"][0]["content"]["parts"][0]["text"].as_str() {
                let cleaned = content
                    .trim()
                    .trim_start_matches("```json")
                    .trim_start_matches("```")
                    .trim_end_matches("```")
                    .trim();
                return Ok(cleaned.to_string());
            } else {
                errors.push(format!("Empty response from {}", model));
            }
        } else {
            let status = resp.status();
            let error_text = resp.text().await.unwrap_or_default();
            errors.push(format!(
                "API Error ({}): {} - {}",
                model, status, error_text
            ));
            continue;
        }
    }

    Err(format!("AI ì—°ê²° ì‹¤íŒ¨:\n{}", errors.join("\n")))
}

#[command]
pub async fn parse_business_card_ai(
    app: AppHandle,
    image_base64: String,
    mime_type: String,
) -> Result<ParsedBusinessCard, String> {
    let api_key = get_gemini_api_key(&app).ok_or("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")?;

    let prompt = "
    Analyze this business card image.
    Extract: name, mobile (010-xxxx-xxxx format), phone, email, company, job_title, address.
    Put everything else useful in 'memo'.
    Return JSON only with keys: name, mobile, phone, email, company, job_title, address, memo.
    Use null for missing fields.
    ";

    let json_str = call_gemini_vision_ai(&api_key, prompt, &image_base64, &mime_type).await?;

    let result: ParsedBusinessCard = serde_json::from_str(&json_str)
        .map_err(|e| format!("Parsing Error: {}. Raw: {}", e, json_str))?;

    Ok(result)
}

#[command]
pub async fn test_gemini_connection(_app: AppHandle) -> Result<String, String> {
    Ok("Connection OK".to_string())
}

#[command]
pub async fn get_ai_behavior_strategy(
    _state: State<'_, DbPool>,
    _customer_id: Option<String>,
) -> Result<String, String> {
    Ok("Behavior Strategy Stub".to_string())
}

#[command]
pub async fn analyze_online_sentiment(_state: State<'_, DbPool>) -> Result<String, String> {
    Ok("Sentiment Analysis Stub".to_string())
}

#[command]
pub async fn get_morning_briefing(
    _app: AppHandle,
    _state: State<'_, DbPool>,
) -> Result<String, String> {
    Ok("Morning Briefing Stub".to_string())
}

#[command]
pub async fn get_ai_repurchase_analysis(_state: State<'_, DbPool>) -> Result<String, String> {
    Ok("Repurchase Analysis Stub".to_string())
}

#[command]
pub async fn get_weather_marketing_advice(_state: State<'_, DbPool>) -> Result<String, String> {
    Ok("Weather Advice Stub".to_string())
}

#[command]
pub async fn get_consultation_briefing(
    app: AppHandle,
    state: State<'_, DbPool>,
    customer_id: String,
) -> Result<String, String> {
    let api_key = get_gemini_api_key(&app).ok_or("Gemini API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")?;

    let customer: Option<Customer> =
        sqlx::query_as("SELECT * FROM customers WHERE customer_id = $1")
            .bind(&customer_id)
            .fetch_optional(&*state)
            .await
            .map_err(|e| e.to_string())?;

    let c = customer.ok_or("ê³ ê° ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")?;

    let history: Vec<crate::db::Consultation> = sqlx::query_as(
        "SELECT * FROM consultations WHERE customer_id = $1 ORDER BY consult_date DESC LIMIT 30",
    )
    .bind(&customer_id)
    .fetch_all(&*state)
    .await
    .map_err(|e| e.to_string())?;

    if history.is_empty() {
        return Ok("ì´ì „ ìƒë‹´ ë‚´ì—­ì´ ì—†ëŠ” ì‹ ê·œ ê³ ê°ì…ë‹ˆë‹¤.".to_string());
    }

    let mut context_str = format!(
        "ê³ ê°ëª…: {} ({})\nìƒë‹´ ë‚´ì—­:\n",
        c.customer_name,
        c.membership_level.unwrap_or_default()
    );
    for h in history {
        context_str.push_str(&format!(
            "- [{} / {}] ì œëª©: {} | ë‚´ìš©: {} | ë‹µë³€: {}\n",
            h.consult_date,
            h.category,
            h.title,
            h.content,
            h.answer.unwrap_or_default()
        ));
    }

    let prompt = format!(
        "ë‹¹ì‹ ì€ ìŠ¤ë§ˆíŠ¸ ë†ì¥ì˜ ì „ë¬¸ ìƒë‹´ ê´€ë¦¬ìì…ë‹ˆë‹¤. ì•„ë˜ì˜ ê³ ê° ìƒë‹´ ì´ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ, ìƒë‹´ì›ì´ ì „í™”ë¥¼ ê±¸ê¸° ì „ ì½ì–´ì•¼ í•  'í•µì‹¬ ë¸Œë¦¬í•‘'ì„ 3ì¤„ ë‚´ì™¸ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”. ì´ ê³ ê°ì˜ ì„±í–¥, ê³¼ê±° ì£¼ìš” ë¬¸ì˜, ì£¼ì˜ì‚¬í•­ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ì •ì¤‘í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n\n\
        {}\n\n\
        **ë¸Œë¦¬í•‘:**",
        context_str
    );

    call_gemini_ai_internal(&api_key, &prompt).await
}

#[command]
pub async fn get_pending_consultations_summary(
    app: AppHandle,
    state: State<'_, DbPool>,
) -> Result<String, String> {
    let api_key = get_gemini_api_key(&app).ok_or("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")?;

    let pending: Vec<crate::db::Consultation> = sqlx::query_as(
        "SELECT * FROM consultations WHERE status != 'ì™„ë£Œ' ORDER BY consult_date DESC LIMIT 50",
    )
    .fetch_all(&*state)
    .await
    .map_err(|e| e.to_string())?;

    if pending.is_empty() {
        return Ok("í˜„ì¬ ì²˜ë¦¬ ëŒ€ê¸° ì¤‘ì¸ ìƒë‹´ì´ ì—†ìŠµë‹ˆë‹¤. í‰í™”ë¡œìš´ í•˜ë£¨ì…ë‹ˆë‹¤! ğŸ˜Š".to_string());
    }

    let mut context = String::new();
    for p in pending {
        context.push_str(&format!(
            "- [{} / {}] ìš°ì„ ìˆœìœ„: {} | ì œëª©: {} | ë‚´ìš©: {}\n",
            p.consult_date, p.category, p.priority, p.title, p.content
        ));
    }

    let prompt = format!(
        "ë‹¹ì‹ ì€ ìŠ¤ë§ˆíŠ¸ ë†ì¥ì˜ ê³ ê° ê´€ë¦¬ ì „ëµê°€ì…ë‹ˆë‹¤. ì•„ë˜ì˜ 'ì²˜ë¦¬ ëŒ€ê¸° ì¤‘ì¸ ìƒë‹´ ë¦¬ìŠ¤íŠ¸'ë¥¼ ë³´ê³  ì‚¬ì¥ë‹˜ì„ ìœ„í•œ 1ë¶„ ìš”ì•½ ë¸Œë¦¬í•‘ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\n\
        [ëŒ€ê¸° ë¦¬ìŠ¤íŠ¸]\n\
        {}\n\n\
        [ì‘ì„± ì§€ì¹¨]\n\
        1. í˜„ì¬ ê°€ì¥ ì‹œê¸‰í•œ ìƒë‹´ í…Œë§ˆê°€ ë¬´ì—‡ì¸ì§€(ì˜ˆ: ë°°ì†¡ ì§€ì—°, ìƒí’ˆ ë¶ˆë§Œ ë“±) íŒŒì•…í•˜ì—¬ ìƒë‹¨ì— ëª…ì‹œí•˜ì„¸ìš”.\n\
        2. ì „ì²´ì ì¸ ìƒë‹´ ê°ì • ìƒíƒœê°€ ì–´ë–¤ì§€ ìš”ì•½í•˜ì„¸ìš”.\n\
        3. ì‚¬ì¥ë‹˜ì´ ì˜¤ëŠ˜ ê°€ì¥ ë¨¼ì € ì±™ê²¨ì•¼ í•  í•µì‹¬ ì•¡ì…˜ í”Œëœì„ 1~2ê°œ ì œì•ˆí•˜ì„¸ìš”.\n\
        4. HTML í˜•ì‹ìœ¼ë¡œ ê¹”ë”í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš” (div, p, ul, li, span ë“± ì‚¬ìš©, ğŸ’¡ ì´ëª¨ì§€ í™œìš©).\n\
        5. ì •ì¤‘í•˜ê³  í™œê¸°ì°¬ í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.",
        context
    );

    call_gemini_ai_internal(&api_key, &prompt).await
}

#[command]
pub async fn get_ai_marketing_proposal(_state: State<'_, DbPool>) -> Result<String, String> {
    Ok("AI Marketing Proposal Stub".to_string())
}

#[command]
pub async fn get_ai_detailed_plan(
    _state: State<'_, DbPool>,
    _plan_type: String,
) -> Result<String, String> {
    Ok("AI Detailed Plan Stub".to_string())
}

#[command]
pub async fn get_consultation_ai_advisor(
    _state: State<'_, DbPool>,
    _consultation_id: i32,
) -> Result<String, String> {
    Ok("Consultation Advisor Stub".to_string())
}

#[command]
pub async fn get_ai_consultation_advice(
    _state: State<'_, DbPool>,
    _consultation_id: i32,
) -> Result<String, String> {
    Ok("Consultation Advice Stub".to_string())
}

#[command]
pub async fn get_ai_demand_forecast(_state: State<'_, DbPool>) -> Result<String, String> {
    Ok("Demand Forecast Stub".to_string())
}
